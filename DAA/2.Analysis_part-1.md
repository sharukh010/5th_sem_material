# Analysis of Algorithms

#### Parameters
When analyzing an algorithm, several key parameters are considered:

1. **Time Complexity**: Measures the amount of time an algorithm takes to complete as a function of the length of the input.
2. **Space Complexity**: Measures the amount of memory an algorithm uses as a function of the length of the input.

#### Design Techniques of Algorithms
There are various techniques used to design algorithms. Here are a few common ones:

1. **Brute Force**: Directly solves the problem in the simplest way, often through exhaustive search.
   - **Example**: Linear search.

2. **Divide and Conquer**: Breaks the problem into smaller sub-problems, solves each sub-problem recursively, and then combines the results.
   - **Example**: Merge sort, quicksort.

3. **Dynamic Programming**: Solves problems by breaking them down into simpler sub-problems and storing the results of these sub-problems to avoid redundant work.
   - **Example**: Fibonacci sequence.

4. **Greedy Algorithms**: Makes the locally optimal choice at each step with the hope of finding the global optimum.
   - **Example**: Kruskal’s algorithm for finding the minimum spanning tree.

5. **Backtracking**: Tries to build a solution incrementally and removes solutions that fail to satisfy the constraints at any point.
   - **Example**: N-Queens problem.

6. **Recursive Algorithms**: Solves a problem by solving smaller instances of the same problem.
   - **Example**: Factorial calculation.

#### Asymptotic Analysis
Asymptotic analysis is used to describe the behavior of an algorithm as the input size grows. It helps compare the efficiency of different algorithms.

1. **Big O Notation (O)**: Describes the upper bound of the time complexity. It represents the worst-case scenario.
   - **Example**: If an algorithm has a time complexity of O(n^2), its running time increases quadratically as the input size grows.

2. **Big Omega Notation (Ω)**: Describes the lower bound of the time complexity. It represents the best-case scenario.
   - **Example**: If an algorithm has a time complexity of Ω(n), its running time grows linearly with the input size in the best case.

3. **Big Theta Notation (Θ)**: Describes the tight bound of the time complexity. It represents the average-case scenario where the algorithm's running time is bound both above and below by the same function.
   - **Example**: If an algorithm has a time complexity of Θ(n log n), its running time grows proportionally to n log n for all cases.

#### Lower Bound, Upper Bound, and Tight Bound

- **Lower Bound**: The minimum time an algorithm can take for any input size. Represented by Ω (Big Omega).
- **Upper Bound**: The maximum time an algorithm can take for any input size. Represented by O (Big O).
- **Tight Bound**: The exact bound of an algorithm's running time, representing the average-case scenario. Represented by Θ (Big Theta).

#### Best Case, Worst Case, and Average Case
When analyzing an algorithm, we consider different scenarios:

1. **Best Case**: The scenario where the algorithm performs the fewest steps. Represented by Ω.
   - **Example**: For linear search, the best case is when the element to be found is the first element.

2. **Worst Case**: The scenario where the algorithm performs the maximum steps. Represented by O.
   - **Example**: For linear search, the worst case is when the element to be found is the last element or not in the list at all.

3. **Average Case**: The scenario where the algorithm performs an average number of steps, considering all possible inputs. Represented by Θ.
   - **Example**: For linear search, the average case would be when the element is somewhere in the middle of the list.

### Examples of Asymptotic Notations

1. **Linear Search**:
   - Best Case: Ω(1) – Element found at the first position.
   - Worst Case: O(n) – Element not found or found at the last position.
   - Average Case: Θ(n/2) – On average, element found in the middle of the list.

2. **Binary Search**:
   - Best Case: Ω(1) – Element found at the middle position in the first comparison.
   - Worst Case: O(log n) – Element found after log n comparisons.
   - Average Case: Θ(log n) – On average, element found after log n comparisons.

3. **Merge Sort**:
   - Best Case: Ω(n log n) – Even in the best case, merge sort performs n log n operations.
   - Worst Case: O(n log n) – Merge sort always performs n log n operations in the worst case.
   - Average Case: Θ(n log n) – On average, merge sort performs n log n operations.

### Summary

- **Parameters**: Time complexity, space complexity.
- **Design Techniques**: Brute force, divide and conquer, dynamic programming, greedy algorithms, backtracking, recursive algorithms.
- **Asymptotic Analysis**: Big O (upper bound), Big Omega (lower bound), Big Theta (tight bound).
- **Bounds**: Lower bound (Ω), upper bound (O), tight bound (Θ).
- **Cases**: Best case (Ω), worst case (O), average case (Θ).

Understanding these concepts will help you analyze and compare different algorithms effectively, which is crucial for optimizing solutions and performing well in your descriptive exam.